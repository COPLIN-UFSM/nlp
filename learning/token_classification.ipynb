{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Classificação de tokens",
   "id": "4c9df7156a33be32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Carrega bibliotecas",
   "id": "4c04b78e603c45a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T10:45:17.758705Z",
     "start_time": "2024-11-27T10:45:13.999699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pprint\n",
    "\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer, EvalPrediction"
   ],
   "id": "b01e126e783df2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define arquivo de parâmetros\n",
    "\n",
    "* Se existir um arquivo em `learning/instance/parameters.json`, usa este arquivo;\n",
    "* Se não existir, usa as definições-padrão abaixo"
   ],
   "id": "4dffdb96a5564908"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T11:58:46.434631Z",
     "start_time": "2024-11-27T11:58:46.427476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "current_path = os.path.abspath('.')\n",
    "\n",
    "if os.path.basename(current_path) != 'learning':\n",
    "    raise FileNotFoundError('Execute este notebook a partir da pasta \\'learning\\'!')\n",
    "\n",
    "parameters_path = os.path.join(current_path, 'instance', 'parameters.json')\n",
    "\n",
    "if os.path.exists(parameters_path):\n",
    "    status = f'usando arquivo de parâmetros em {parameters_path}'\n",
    "    with open(parameters_path, 'r', encoding='utf-8') as read_file:\n",
    "        parameters = json.load(read_file)\n",
    "else:\n",
    "    status = (f'arquivo com parâmetros não encontrado em {parameters_path}'\n",
    "              f'usando definições do próprio notebook')\n",
    "    \n",
    "    parameters = {\n",
    "        \"model_name\": \"neuralmind/bert-base-portuguese-cased\",\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"use_cpu\": False,\n",
    "        \"repo_owner\": \"COPLIN-UFSM\",\n",
    "        \"repo_name\": \"nlp-data\",\n",
    "        \"remote_dataset_path\": 'data/token_classification/input/annotated.jsonl',\n",
    "        \"local_dataset_path\": r'C:\\Users\\henry\\Projects\\COPLIN-UFSM\\nlp\\learning\\instance\\annotated.jsonl',\n",
    "        \"input_column\": \"text\",\n",
    "        \"val_size\": 0.2,\n",
    "        \"output_dir\": \"instance/models\",\n",
    "        \"output_model_name\": \"student-token-classification\",\n",
    "        \"batch_size\": 128,\n",
    "        \"optim\": \"adamw_torch\",\n",
    "        \"problem_type\": \"token_classification\",\n",
    "        \"max_length\": 128,\n",
    "        \"class_name\": [\"positive\", \"negative\"],\n",
    "        \"auto_find_batch_size\": True,\n",
    "        \"push_to_hub\": False,\n",
    "        \"github_access_token\": None\n",
    "    }\n",
    "\n",
    "print(f'\\n{status}')"
   ],
   "id": "1aa1c43af6e3f5db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "usando arquivo de parâmetros em C:\\Users\\henry\\Projects\\COPLIN-UFSM\\nlp\\learning\\instance\\parameters.json\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Carrega dataset\n",
    "\n",
    "* Se o dataset não estiver presente, armazena-o nos arquivos temporários do computador\n",
    "* Se estiver presente, verifica o MD5 para garantir que é o mesmo armazenado remotamente\n",
    "* Se estiver presente e não for o mesmo remoto, baixa novamente"
   ],
   "id": "f1f5202e71eaa74b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T11:02:09.355993Z",
     "start_time": "2024-11-27T11:02:08.995206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "def get_dataset(parameters):\n",
    "    def __do_request__(path):\n",
    "        return requests.get(\n",
    "            f'https://api.github.com/repos/{owner}/{repo}/contents/{path}',\n",
    "            headers={\n",
    "                'accept': 'application/vnd.github.v3.raw',\n",
    "                'authorization': f'token {token}'\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # parâmetros da requisição\n",
    "    owner = parameters['repo_owner']\n",
    "    repo = parameters['repo_name']\n",
    "    local_dataset_path = parameters['local_dataset_path']\n",
    "    remote_dataset_path = parameters['remote_dataset_path']\n",
    "    md5_remote_path = remote_dataset_path[:remote_dataset_path.index('.')] + '.md5'\n",
    "    md5_local_path = local_dataset_path[:local_dataset_path.index('.')] + '.md5'\n",
    "    token = parameters['github_access_token'] \n",
    "    \n",
    "    # solicita o md5 do dataset antes\n",
    "    md5_remote_contents = __do_request__(md5_remote_path).text\n",
    "    if os.path.exists(md5_local_path):\n",
    "        with open(md5_local_path, 'r') as read_file:\n",
    "            md5_local_contents = read_file.read()\n",
    "    else:\n",
    "        md5_local_contents = None\n",
    "    \n",
    "    print(f' local md5: {md5_local_contents}')\n",
    "    print(f'remote md5: {md5_remote_contents}')\n",
    "        \n",
    "    if md5_local_contents != md5_remote_contents:\n",
    "        print('md5 não compatível; Lendo dataset do repositório do GitHub')\n",
    "        # escreve md5 remoto\n",
    "        with open(md5_local_path, 'w') as write_file:\n",
    "            write_file.write(md5_remote_contents)\n",
    "      \n",
    "        # converte texto em dataframe\n",
    "        df = pd.read_json(StringIO(__do_request__(remote_dataset_path).text), lines=True)\n",
    "        # escreve dataframe em disco\n",
    "        with open(os.path.join('instance', parameters['local_dataset_path']), 'w', encoding='utf-8') as write_file:\n",
    "            for i, row in df.iterrows():\n",
    "                write_file.write(row.to_json(force_ascii=False) + '\\n')    \n",
    "    else:\n",
    "        # lê dataframe do disco\n",
    "        print('md5 compatível; Lendo dataset do disco local')\n",
    "        df = pd.read_json(parameters['local_dataset_path'], lines=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "df = get_dataset(parameters)\n",
    "display(df)"
   ],
   "id": "b4d97a695ff6d8ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " local md5: fafc3427ee648e28a253f214a2976c43\n",
      "remote md5: fafc3427ee648e28a253f214a2976c43\n",
      "md5 compatível; Lendo dataset do disco local\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "          id                                               text  ID_REFEICAO  \\\n",
       "0      29875  o feijão possui muitos gases devido ao método ...     15269107   \n",
       "1      29876  poucos legumes e o arroz poderia estar um pouc...     15269042   \n",
       "2      29877  a comida estava boa e com a marmita do almoço,...     15268993   \n",
       "3      29878          muito arroz. já esta virando desperdício.     15269191   \n",
       "4      29879  a batata poderia ser mais cozida, estava crua ...     15268862   \n",
       "...      ...                                                ...          ...   \n",
       "10923  40798      sou vegana e tinha ovo como opção de proteína     17905614   \n",
       "10924  40799  por favor quando servirem lentilha, sirvam fei...     17897771   \n",
       "10925  40800  execelente café, o cafe poderia ser disponibil...     17622301   \n",
       "10926  40801                             faltou sal na galinha.     17625116   \n",
       "10927  40802                        falta muito o molho de alho     17634913   \n",
       "\n",
       "       Avaliação Variedade e Qualidade  Avaliação Atendimento  \\\n",
       "0                                    3                      5   \n",
       "1                                    3                      5   \n",
       "2                                    5                      5   \n",
       "3                                    3                      3   \n",
       "4                                    4                      5   \n",
       "...                                ...                    ...   \n",
       "10923                                1                      4   \n",
       "10924                                2                      5   \n",
       "10925                                5                      5   \n",
       "10926                                5                      5   \n",
       "10927                                5                      5   \n",
       "\n",
       "       Avaliação Organização e Limpeza  \\\n",
       "0                                    5   \n",
       "1                                    5   \n",
       "2                                    5   \n",
       "3                                    3   \n",
       "4                                    5   \n",
       "...                                ...   \n",
       "10923                                3   \n",
       "10924                                5   \n",
       "10925                                5   \n",
       "10926                                5   \n",
       "10927                                5   \n",
       "\n",
       "                                                   label Comments  \n",
       "0          [[0, 111, comida/bebida], [0, 111, negativo]]       []  \n",
       "1      [[0, 59, comida/bebida], [0, 59, negativo], [6...       []  \n",
       "2            [[0, 63, comida/bebida], [0, 63, positivo]]       []  \n",
       "3              [[0, 41, negativo], [0, 41, organização]]       []  \n",
       "4      [[0, 51, negativo], [0, 52, comida/bebida], [5...       []  \n",
       "...                                                  ...      ...  \n",
       "10923        [[0, 45, negativo], [0, 45, outras dietas]]       []  \n",
       "10924  [[0, 56, comida/bebida], [0, 56, negativo], [0...       []  \n",
       "10925  [[0, 16, comida/bebida], [0, 17, positivo], [1...       []  \n",
       "10926        [[0, 22, comida/bebida], [0, 22, negativo]]       []  \n",
       "10927          [[0, 27, negativo], [0, 27, organização]]       []  \n",
       "\n",
       "[10928 rows x 8 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>ID_REFEICAO</th>\n",
       "      <th>Avaliação Variedade e Qualidade</th>\n",
       "      <th>Avaliação Atendimento</th>\n",
       "      <th>Avaliação Organização e Limpeza</th>\n",
       "      <th>label</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29875</td>\n",
       "      <td>o feijão possui muitos gases devido ao método ...</td>\n",
       "      <td>15269107</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[[0, 111, comida/bebida], [0, 111, negativo]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29876</td>\n",
       "      <td>poucos legumes e o arroz poderia estar um pouc...</td>\n",
       "      <td>15269042</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[[0, 59, comida/bebida], [0, 59, negativo], [6...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29877</td>\n",
       "      <td>a comida estava boa e com a marmita do almoço,...</td>\n",
       "      <td>15268993</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[[0, 63, comida/bebida], [0, 63, positivo]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29878</td>\n",
       "      <td>muito arroz. já esta virando desperdício.</td>\n",
       "      <td>15269191</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[[0, 41, negativo], [0, 41, organização]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29879</td>\n",
       "      <td>a batata poderia ser mais cozida, estava crua ...</td>\n",
       "      <td>15268862</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[[0, 51, negativo], [0, 52, comida/bebida], [5...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10923</th>\n",
       "      <td>40798</td>\n",
       "      <td>sou vegana e tinha ovo como opção de proteína</td>\n",
       "      <td>17905614</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>[[0, 45, negativo], [0, 45, outras dietas]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10924</th>\n",
       "      <td>40799</td>\n",
       "      <td>por favor quando servirem lentilha, sirvam fei...</td>\n",
       "      <td>17897771</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[[0, 56, comida/bebida], [0, 56, negativo], [0...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10925</th>\n",
       "      <td>40800</td>\n",
       "      <td>execelente café, o cafe poderia ser disponibil...</td>\n",
       "      <td>17622301</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[[0, 16, comida/bebida], [0, 17, positivo], [1...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10926</th>\n",
       "      <td>40801</td>\n",
       "      <td>faltou sal na galinha.</td>\n",
       "      <td>17625116</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[[0, 22, comida/bebida], [0, 22, negativo]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10927</th>\n",
       "      <td>40802</td>\n",
       "      <td>falta muito o molho de alho</td>\n",
       "      <td>17634913</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[[0, 27, negativo], [0, 27, organização]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10928 rows × 8 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define funções de cálculo de métricas",
   "id": "28659058e85246e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calcula métricas de um modelo multi-rótulo. Adaptação do código-fonte de\n",
    "        https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "    \"\"\"\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average='micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {\n",
    "       'roc_auc': roc_auc,\n",
    "       'accuracy': accuracy\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction) -> dict:\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds,\n",
    "        labels=p.label_ids\n",
    "    )\n",
    "    return result"
   ],
   "id": "b4921e9ce5a2a7cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_dataset_metadata(df: pd.DataFrame, class_labels: str | list) -> tuple:\n",
    "    \"\"\"\n",
    "    Coleta metadados de um DataFrame.\n",
    "\n",
    "    :param df: DataFrame para o qual os metadados serão coletados\n",
    "    :param class_labels: Um dos dois: o nome da coluna no DataFrame com os rótulos, ou uma lista com o nome dos rótulos\n",
    "    :return: Uma tupla com os seguintes itens: dataframe (pd.DataFrame), número de rótulos (int), lista com rótulos\n",
    "        (list), nome da coluna com o atributo classe (str), label2id (dict), id2label (dict)\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(class_labels, str):\n",
    "        labels = sorted(df[class_labels].unique())  # type: list\n",
    "    elif isinstance(class_labels, list):\n",
    "        labels = sorted(class_labels)\n",
    "    else:\n",
    "        raise TypeError('Tipo desconhecido para o parâmetro class_name (deve ser str ou list)')\n",
    "\n",
    "    label2id = {k: i for i, k in enumerate(labels)}\n",
    "    id2label = {i: k for k, i in label2id.items()}\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    return num_labels, labels, label2id, id2label\n",
    "\n",
    "\n",
    "def preprocess_data(\n",
    "        dataset: Dataset, tokenizer: BertTokenizer,\n",
    "        label2id: dict, input_column: str, max_length: int\n",
    "):\n",
    "    # take a batch of texts\n",
    "\n",
    "    text = dataset[input_column]\n",
    "    # encode them\n",
    "    encoding = tokenizer(text, padding='max_length', truncation=True, max_length=max_length)\n",
    "    # add labels\n",
    "    labels_batch = {k: dataset[k] for k in dataset if k in list(label2id.keys())}\n",
    "    # create numpy array of shape (batch_size, num_labels)\n",
    "    labels_matrix = np.zeros((len(text), len(label2id)))\n",
    "    # fill numpy array\n",
    "    for label, idx in label2id.items():\n",
    "        labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "    encoding['label'] = labels_matrix.tolist()\n",
    "\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def tokenize_and_get_metadata(\n",
    "        df: pd.DataFrame, tokenizer: BertTokenizer, input_column: str, classe_name: str | list,\n",
    "        max_length: int, batch_size: int = 8\n",
    "):\n",
    "    num_labels, labels, label2id, id2label = get_dataset_metadata(df, classe_name)\n",
    "\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    tokenized = dataset.map(\n",
    "        lambda x: preprocess_data(x, tokenizer, label2id, input_column, max_length),\n",
    "        batched=True, batch_size=batch_size\n",
    "    )\n",
    "    tokenized.set_format('torch')\n",
    "    return tokenized, num_labels, labels, label2id, id2label\n",
    "\n",
    "\n",
    "def tokenize_datasets(tokenizer: BertTokenizer, original_sets: dict, parameters: dict) -> tuple:\n",
    "    tokenized_sets = {}\n",
    "    num_labels = None\n",
    "    labels = None\n",
    "    label2id = None\n",
    "    id2label = None\n",
    "    for name in original_sets.keys():\n",
    "        tokenized_sets[name], num_labels, labels, label2id, id2label = tokenize_and_get_metadata(\n",
    "            original_sets[name], tokenizer, parameters['input_column'], parameters['class_name'],\n",
    "            parameters['max_length'], parameters['batch_size']\n",
    "        )\n",
    "\n",
    "    return tokenized_sets, num_labels, labels, label2id, id2label\n",
    "\n",
    "\n",
    "def do_train_model(\n",
    "    tokenizer: BertTokenizer, parameters: dict, tokenized_sets: dict, num_labels: int, id2label: dict, label2id: dict\n",
    ") -> tuple:\n",
    "    \n",
    "\n",
    "    return tokenizer, trainer\n",
    "\n",
    "\n",
    "def evaluate_on_test_set(trainer: Trainer, test_set=None) -> dict:\n",
    "    \"\"\"\n",
    "    Avalia desempenho do modelo no conjunto de teste, se este existir\n",
    "\n",
    "    :param trainer: Treinador do modelo\n",
    "    :param test_set: Conjunto de teste\n",
    "    :return: Um dicionário com as métricas de desempenho\n",
    "    \"\"\"\n",
    "    if test_set is not None:\n",
    "        res = trainer.evaluate(test_set)\n",
    "        print('Resultados no conjunto de teste:')\n",
    "        for k, v in res.items():\n",
    "            print(f'{k}: {v}')\n",
    "        return res\n",
    "    return {}\n",
    "\n",
    "\n",
    "def main(parameters, original_sets) -> None:\n",
    "    device = get_device(parameters['use_cpu'])\n",
    "    print(f'using {device} as device')\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(parameters['model_name'], do_lower_case=False)  # type: BertTokenizer\n",
    "    tokenized_sets, num_labels, labels, label2id, id2label = tokenize_datasets(tokenizer, original_sets, parameters)\n",
    "    tokenizer, trainer = do_train_model(tokenizer, parameters, tokenized_sets, num_labels, id2label, label2id)\n",
    "    tokenizer.save_pretrained(os.path.join(parameters['output_dir'], parameters['output_model_name']))\n",
    "    trainer.save_model(os.path.join(parameters['output_dir'], parameters['output_model_name']))\n",
    "    evaluate_on_test_set(trainer, tokenized_sets['test'])"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pega dispositivo onde o treinamento será executado",
   "id": "e1e67b8f4e81e982"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_device(use_cpu: bool = False) -> str:\n",
    "    if not use_cpu:\n",
    "        device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "\n",
    "    return device"
   ],
   "id": "e0cb68ec2cf35a2d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Treina modelo",
   "id": "62c7cc36a45ecc8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# carrega um modelo pré-treinado com uma camada totalmente conectada nova no fim do modelo\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    parameters['model_name'], num_labels=num_labels, id2label=id2label, label2id=label2id,\n",
    "    problem_type=parameters['problem_type']\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=parameters['output_dir'],\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=parameters['num_train_epochs'],\n",
    "    # use_cpu=parameters['use_cpu'],\n",
    "    optim=parameters['optim'],\n",
    "    load_best_model_at_end=True,\n",
    "    auto_find_batch_size=parameters['auto_find_batch_size'],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_sets['train'],\n",
    "    eval_dataset=tokenized_sets['val'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "id": "6406723ed560fd85"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
